Multi-layer Perceptron

Objective:
Write a Program to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function.

Description:
The implemented Multi-Layer Perceptron (MLP) model consists of the following components:
1.	Input Layer:
  o	Two input neurons correspond to the features of the XOR truth table ([0, 0], [0, 1], [1, 0], [1, 1]).
2.	Hidden Layer:
  o	Contains two neurons to capture the non-linear decision boundaries required to learn the XOR function.
  o	Uses manually chosen weights and biases.
  o	A step activation function is applied for binary output (0,1).
3.	Output Layer:
  o	One neuron takes input from the hidden layer and applies the step activation function to produce binary output for XOR.
4.	Learning Process:
  o	Uses weighted sum and bias and applies the step activation function to obtain the binary outputs.


Description of code:
1.	Importing the numpy library.
2.	The step function is used as an activation function, 
      If x >= 0, then output = 1
      If x < 0, then output = 0
3.	XOR dataset is initialized 
      X = [(0,0),(0,1),(1,0),(1,1)]
      Y = [0,1,1,0]
4.	Weights and biases are initialized for neurons -> 2 neurons in the hidden layer and 1 in the output layer.
5.	The perceptron_forward function takes weights, biases, and one input from X as parameters and applies a step function to them.
6.	In the mlp_xor_predict function, first hidden output is calculated using perceptron_forward function and this output works as input for the output layer where perceptron_forward function is applied again to generate the final output.
7.	Printing the final output. 

Output:
Input	Output
[0 0]	0
[0 1]	1
[1 0]	1
[1 1]	0

Performance:
-	Accurately predicts the XOR outputs.

My comments:
-	Instead of choosing the weights and biases manually, we can use learning algorithms.
-	We can use backpropagation to automatically update the weights and biases.
